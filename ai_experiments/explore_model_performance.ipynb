{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736e9414-bc4e-4a38-ba6b-815271f0044e",
   "metadata": {},
   "source": [
    "# Explore model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce66db-4d9a-4061-88cd-4730954f1483",
   "metadata": {},
   "source": [
    "This document presents an overview of the results from a variety of model outputs, including further modelling chip probabilities.\n",
    "\n",
    "There are several aspects of the predictions we'd like to explore and compare across models:\n",
    "\n",
    "- Overall metrics:\n",
    "     - [ ] Accuracy ([micro-F1](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1): general over-simplified measure of how many chips the model got right\n",
    "     - [ ] [Kappa score](https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c): measure of how many chips the model got right, _above_ what it would have got by pure chance\n",
    "- Class-based metrics:\n",
    "     - [ ] Accuracy: proportion of chips the model got right for each class\n",
    "     - [ ] Macro-F1: overview of how good the model is at predicting each single class\n",
    "- Confusion matrices: more disaggregated view on predictions. Detailed but not summarising.\n",
    "- Spatial metrics: compare whether the distribution of predicted labels over space resembles that of the true values\n",
    "     - [ ] Joint count statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
