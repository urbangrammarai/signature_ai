{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800be99a-b490-4c6a-8d5d-06a7d5bce612",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "Pipeline testing the performance of different models on the NW sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfcb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from model_builder import model_builder, relabel, class_merger, balancer\n",
    "\n",
    "import model_builder\n",
    "import tools_keras\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456789e0",
   "metadata": {},
   "source": [
    "Load chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0131505",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = {\n",
    "    'tensor': (\n",
    "        '../urbangrammar_samba/'\n",
    "        'spatial_signatures/chips/sample.npz'\n",
    "    ),\n",
    "    'folder': \"../urbangrammar_samba/spatial_signatures/ai/nw_32/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c061e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.load(specs[\"tensor\"], allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4fefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chips = data[\"chips\"]\n",
    "labels = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f35ee5",
   "metadata": {},
   "source": [
    "Shuffle data to ensure full randomness before splitting to test, validation and secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f5bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(0)\n",
    "\n",
    "shuffled_idx = numpy.arange(0, chips.shape[0])\n",
    "numpy.random.shuffle(shuffled_idx)\n",
    "\n",
    "chips = chips[shuffled_idx]\n",
    "labels = labels[shuffled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18de7ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0_0', '1_0', '2_0', '2_1', '2_2', '3_0', '4_0', '5_0', '6_0',\n",
       "        '7_0', '8_0', '9_0', '9_2', '9_4'], dtype=object),\n",
       " array([25571,  1576,   379,   243,   255,  4827,  3090,  2114,   271,\n",
       "        26610,    76,    18,    11,     1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05bafe",
   "metadata": {},
   "source": [
    "Merge groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b3712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mapping = [\n",
    "    ['9_0', '9_1', '9_2', '9_4', '9_5', '2_0', '2_1', '2_2'],\n",
    "    ['1_0', '3_0', '5_0', '6_0', '8_0'],\n",
    "    ['0_0', '4_0', '7_0']\n",
    "]\n",
    "labels = class_merger(labels, group_mapping)\n",
    "\n",
    "# If you don't want to merge groups, use `relabel` to ensure your labels are in the proper format\n",
    "# labels = relabel(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5dd894",
   "metadata": {},
   "source": [
    "Drop chips from excessive classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "599324f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of selected chips: 18841 out of 65042\n",
      "Counts:\n",
      " {0: 907, 1: 8864, 2: 9070}\n"
     ]
    }
   ],
   "source": [
    "mask = balancer(labels, max_ratio=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee75d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[mask]\n",
    "chips = chips[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7ad3b",
   "metadata": {},
   "source": [
    "Rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f95c909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chips = chips / (chips.max() / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabef3e",
   "metadata": {},
   "source": [
    "Split data into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d4bd5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "split_1 = int(chips.shape[0] * 0.6)\n",
    "split_2 = int(chips.shape[0] * 0.8)\n",
    "\n",
    "\n",
    "# assert that all labels are present in train and validation datasets\n",
    "assert (numpy.unique(labels[:split_1]) == numpy.unique(labels[split_1:split_2])).all()\n",
    "assert (numpy.unique(labels[:split_1]) == numpy.unique(labels[split_2:])).all()\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((chips[:split_1], labels[:split_1]))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((chips[split_1:split_2], labels[split_1:split_2]))\n",
    "secret_dataset = tf.data.Dataset.from_tensor_slices((chips[split_2:], labels[split_2:]))\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "validation_dataset = validation_dataset.batch(batch_size=batch_size)\n",
    "secret_dataset = secret_dataset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312589b9",
   "metadata": {},
   "source": [
    "Create model and fit in a loop over options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1ff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19_pooling_256_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 8, 8, 3)]         0         \n",
      "_________________________________________________________________\n",
      "resizing_6 (Resizing)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_2 ( (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.nn.bias_add_2 (TFOpLambda (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg19 (Functional)           (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 20,156,483\n",
      "Trainable params: 132,099\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:12:36.273098: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-17 16:12:36.273153: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-17 16:12:36.482919: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-17 16:12:36.483057: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "   1/2565 [..............................] - ETA: 32:20 - loss: 1.7970 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:12:37.487387: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-17 16:12:37.487419: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/2565 [..............................] - ETA: 17:08 - loss: 1.4709 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:12:38.528309: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-17 16:12:38.533468: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n",
      "2021-12-17 16:12:38.623424: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 176 callback api events and 173 activity events. \n",
      "2021-12-17 16:12:38.635550: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-17 16:12:38.696224: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38\n",
      "\n",
      "2021-12-17 16:12:38.715968: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3/2565 [..............................] - ETA: 31:25 - loss: 1.3250 - accuracy: 0.3854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:12:38.738771: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38\n",
      "\n",
      "2021-12-17 16:12:38.753362: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.memory_profile.json.gz\n",
      "2021-12-17 16:12:38.796317: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38\n",
      "Dumped tool data for xplane.pb to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ../urbangrammar_samba/spatial_signatures/ai/nw_8/logs/vgg19_pooling_256_3/train/plugins/profile/2021_12_17_16_12_38/985b7fd52aa6.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2565/2565 [==============================] - 333s 129ms/step - loss: 0.8870 - accuracy: 0.5651 - val_loss: 0.8614 - val_accuracy: 0.5747\n",
      "Epoch 2/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8590 - accuracy: 0.5799 - val_loss: 0.8534 - val_accuracy: 0.5758\n",
      "Epoch 3/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8506 - accuracy: 0.5838 - val_loss: 0.8486 - val_accuracy: 0.5765\n",
      "Epoch 4/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8461 - accuracy: 0.5851 - val_loss: 0.8454 - val_accuracy: 0.5801\n",
      "Epoch 5/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8432 - accuracy: 0.5866 - val_loss: 0.8424 - val_accuracy: 0.5848\n",
      "Epoch 6/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8400 - accuracy: 0.5878 - val_loss: 0.8415 - val_accuracy: 0.5837\n",
      "Epoch 7/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8382 - accuracy: 0.5891 - val_loss: 0.8422 - val_accuracy: 0.5842\n",
      "Epoch 8/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8363 - accuracy: 0.5896 - val_loss: 0.8401 - val_accuracy: 0.5836\n",
      "Epoch 9/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8341 - accuracy: 0.5910 - val_loss: 0.8395 - val_accuracy: 0.5849\n",
      "Epoch 10/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8325 - accuracy: 0.5916 - val_loss: 0.8390 - val_accuracy: 0.5842\n",
      "Epoch 11/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8309 - accuracy: 0.5918 - val_loss: 0.8398 - val_accuracy: 0.5821\n",
      "Epoch 12/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8295 - accuracy: 0.5934 - val_loss: 0.8389 - val_accuracy: 0.5841\n",
      "Epoch 13/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8281 - accuracy: 0.5931 - val_loss: 0.8386 - val_accuracy: 0.5861\n",
      "Epoch 14/250\n",
      "2565/2565 [==============================] - 334s 130ms/step - loss: 0.8265 - accuracy: 0.5943 - val_loss: 0.8384 - val_accuracy: 0.5864\n",
      "Epoch 00014: early stopping\n",
      "time elapsed:    4698.6s\n",
      "prediction saved\n",
      "INFO:tensorflow:Assets written to: ../urbangrammar_samba/spatial_signatures/ai/nw_8/model/vgg19_pooling_256_3/assets\n",
      "perf_model_accuracy for train: 0.5947611927986145\n"
     ]
    }
   ],
   "source": [
    "model_specs = {\n",
    "    'meta_class_map': group_mapping,\n",
    "    'meta_class_names': [\"centres\", \"periphery\", \"countryside\"],\n",
    "    'meta_chip_size': 8,\n",
    "}\n",
    "\n",
    "for model_name, bridge, neurons in product([\"vgg19\", \"efficientnet\"], [\"pooling\"], [256]):\n",
    "    model = model_builder.model_builder(\n",
    "        model_name=model_name, \n",
    "        bridge=bridge, \n",
    "        top_layer_neurons=neurons,\n",
    "        n_labels=3,\n",
    "        input_shape=(8, 8, 3),\n",
    "    )\n",
    "\n",
    "    h = tools_keras.fit_phase(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        validation_dataset,\n",
    "        secret_dataset,\n",
    "        log_folder=specs[\"folder\"] + \"logs\",\n",
    "        pred_folder=specs[\"folder\"] + \"pred\",\n",
    "        model_folder=specs[\"folder\"] + \"model\",\n",
    "        json_folder=specs[\"folder\"] + \"json\",\n",
    "        specs=model_specs,\n",
    "        epochs=250,\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "\n",
    "    )\n",
    "    print(model_name, bridge, neurons, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561500b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools_keras.flush(specs[\"folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acc1098f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102234, 15, 15, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6039313c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
